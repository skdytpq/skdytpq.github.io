---
title: 로지스틱회귀
sidebar:
  nav: docs-ko
aside:
  toc: False
categories:
  - 기초통계
key: 20210810
tags: 
  -통계학
use_math: true
---

# Logistic Regression이란?

보통 설명변수와 반응변수의 관계를 생각할 때, 선형회귀 알고리즘에 익숙한 우리는 반응변수 $Y$ 를 수치형으로 생각한다.

하지만 만약 출력값인 $Y$​가 범주형이라면 우리는 입력값 $X$​를 넣어 여러 클래스 중 알맞은 클래스로 $Y$​를 분류하는 것을 기대할 것이다.

다시말해, input 으로 $X$값을 집어넣어 $Y$값을 수치로 예측하는 것이 아닌, input 으로 $X$를 집어넣었을 때 $Y$는 어떻게 분류되는지에 대해 알아보고자 하는 것이다.

# 선형회귀에서의 작업

---
단순 선형회귀에서 기존에 우리가 했던 작업은 $y_i = \beta_0 + \beta_1X_i+ \epsilon_i$​ 라는 회귀식에서 출발한다.

위의 식은 기존 모집단의 회귀식 $Y = \beta_0 + \beta_1  X$​ 에서 관측값에 대한 오차 $\epsilon$​ 을 추가한 것인데
우리는 이 오차에 대한 가정을 $N(0,\sigma^2)$​을 따른다고 하기 때문에 회귀계수 추정을 위한 관측값들의 추정 회귀식에서

기댓값을 취하면 $E[Y_i] = \beta_0 + \beta_1X_i$​가 된다. 한편 여기서 $X_i$​들은 상수이므로 (관측값) $Y_i$​의 분산역시 $\sigma^2$​으로 주어진다.

따라서 $y_i$​는 $X=x_i$​로 주어질 때 평균이  $\beta_0 + \beta_1X_i$​ 이고 분산이  $\sigma^2$​인 **확률변수이다**

그렇다면 이 회귀식에서 적절한 해석은 어떻게 해야하는가?

intercept인 $\beta_0$는 $y=0$일때의 $Y$의 평균값이며 $\beta_1$은 $x_i$ 가 1단위 증가할 때 $E[Y]$의 변화량이다
(***위식에서 X가 범주형 변수인 경우에는 $\beta_1$에 대한 해석은 $X = 0 or 1$일 때 평균값의 차이이다!***)

즉, 단순선형회귀에서는 데이터들의 추세를 가장 잘 표현해주는 직선이다.

여기서 우리는 단순선형회귀에서 적절한 $\beta$값을 찾기 위한 강력한 idea인 최소제곱법 $\sum_{i=1}^{n} (y-\hat{y_i})^2$을 이용하는데 자세한 과정은 생략하겠다.

이 과정을 거친 후의 회귀식은 식(1)과 같이 해석이 굉장히 직관적이며 $\beta_0,\beta_1$로만 이루어진 경우에는 시각적으로도 이해하기 쉽다.

그러나 $y$가 범주형일 때 적절한 회귀 직선을 그어 input의 $x$값으로 $y$를 분류하는 것은 꽤 애매한 작업일 수 있다.

그 이유는 기존 선형회귀처럼 직선을 그으면 주어지는 $X$의 값의 범위 제한이 없다면  $Y$값의 범위는 이론적으로 $(-\infty,\infty)$ 가 되는데

실제 우리의 목적은 $y$의 클래스를 분류하는 것이기때문에 분류를 위한 적절한 $Y$ 값을 찾기 어렵기 때문이다.

<p align = "center">
  <img width = "400" src = "https://github.com/skdytpq/skdytpq.github.io/blob/master/_pics/2021-08-11-logistic_2.png?raw=true">
  <br>
  그림 1. 범주형 데이터에 대해 선형회귀 모델을 적용하는 경우
</p>

위 그림에서 보듯이 $y$가 범주형인 데이터에 선형회귀 모델을 fitting 시킨 후 그래프를 그리니 해석하기가 애매하다.

따라서 범주형 데이터에 해석을 진행할 때 좀 더 용이할 수 있게 기존 선형회귀 모델이 아닌 다른 모델을 생각해보는 게 자연스럽다. 그렇기에 데이터를 보는 관점을 조금 뒤집어 보자.

우선 여기서의 $Y_i$​​​​ 에 을 우리는 확률(probability)이란 관점으로 바라보도록 하자. 여기서 $Y$​​는 두가지 경우밖에 없기 때문에 $P(Y_i=1)$​​​을 $\pi_i$​라고 하면$P(Y_i=0)$​은 자연스럽게 $1-\pi_i$​가되며 그렇다면 $E[Y_i]$​는 자연스럽게 $\pi_i$​​가 된다. 

그러면 우리는 $E[Y_i] = \pi_i$​​​ 라는 식을 얻었는데, 앞서  단순 선형회귀에서의 $E[Y_i] = \beta_0 + \beta_1X_i$​​​ 의 식과 연결시킨다면 $\pi_i = \beta_0 + \beta_1X_i$​​​ 가 되는데, 이 식은 왠지 어색해보인다. 왜냐하면 범위가 너무 맞지 않기 때문이다. 따라서 우리는 위 식의 우변의 범위를 한정시켜서 좌변의 식 즉, **확률로써 바라본 $E[Y]$​​​**와의 관계를 만들어 적절한 $\beta$​​​​값을 추정하여 분류를 알맞게 하는 모델을 만들 수 있다. 



# 승산(Odds)에 대하여

우리는 $\pi_i = \beta_0 + \beta_1X_i$​이 식에서 좌변과 우변을 알맞게 조정하기 위해 우선 승산비에 대한 개념을 집고 가자.

승산(Odds)이란 간단히 임의의 사건 $A$​​가 발생하지 않을 확률 대비 일어날 확률의 비율을 뜻하는 개념이다. 

예로 성공과 실패의 경우밖에 없는 어떤 게임 $A$​에서 성공확률은 $p$​라고 할 때 Odds는 $\frac{p}{1-p}$​ 이다. 

<p align = "center">
  <img width = "400" src = "https://github.com/skdytpq/skdytpq.github.io/blob/master/_pics/logistic_2.png?raw=true">
  <br>
  그림 2. p값에 따른 Odds의 변화
</p>

이 odds는  

위 그림과 같이 $p$​​가 0에 수렴한다면 $0$​​로 $p$​​가 1로 수렴한다면 $\infty$​​로 발산할 것이다.

이런 승산비가 왜 중요한 것인가?

우리가 알고자하는  $\pi$​가 임의의 $x$​일 때  odds의 개념을 대입한다면 

​	$odds = \frac{\pi(X=x)}{1-\pi(X=x)}$ 	로 표현 가능하다. 

이 odds에 $log$를 취하면 어떻게 될까? 

$log(odds)$ 는 $log(\frac{\pi(X=x)}{1-\pi(X=x)})$이 되는데 이 함수는 아래 그림에서 보듯 범위가 $(-\infty~\infty)$가 된다.  

<p align = "center">
  <img width = "400" src = "https://github.com/skdytpq/skdytpq.github.io/blob/master/_pics/logistic3.png?raw=true">
  <br>
  그림 3. p값 변화에 따른 log(Odds)의 변화
</p>

우리는 이렇게 $Odds$의 개념을 활용하여 $\pi$값의 범위를 $ \beta_0 + \beta_1X_i$과 맞춰줬다!

 $log(\frac{\pi(X=x)}{1-\pi(X=x)})$ =$  \beta_0 + \beta_1*X_i$  $\dots$ 식(1)

우리는 이제 위의 식을 알게 되었는데 우리는 $X$값에 대한 $Y$의 변화를 알고싶은 것이기에 이 식의 역함수 식을 구한다면 

  $\pi(X=x) = \frac{1}{1+e^(-(\beta_0 + \beta_1X))}$

라는 식을 얻게되고 이 식에서의 $\pi(X=x)$는 $E[Y]$와 같으므로 

  $E[Y] = \frac{1}{1+e^(-(\beta_0 + \beta_1X))}$

라는 결과에 도달하게 된다. 즉, $Y$의 결과가 0과 1밖에 없는 확률변수라고 했을 때 우리는 $\beta$ 와 $X$의 변화에 따라 

$E[Y]$ 즉, $Y$값의 기댓값의 변화를 알게됐다. 

다만 식(2)만 놓고 보았을 때는 $\beta$에 대한 해석이 애매할 수 있지만 식(1)의 과정에서 $\beta$의 의미를 본다면 $\beta$의 계수는 

$\pi(X=x)$의 확률을 가진 $Y$의 $log(승산비)$의 영향을 주는구나라고 좀 더 직관적으로 해석이 가능하다. 즉, $\beta$값이 

증가한다면 실패확률 대비 성공확률의 로그비만큼 늘어나는 것이다.

$Y$에 대한 이 확률밀도함수를 보통 로지스틱 함수라고 하는데 이 모양은 앞서 식의 $X$축과$Y$축을 뒤집은 모양이며 

앞서 우리가 $E[Y]$에 대해 적당한 변형을 찾아보자는 흐름에서 이 함수를 도출한 것과 같이 다른 함수를 선택할 

수도 있지만 굳이 sigmoid 함수를 쓰는 이유는 독립변수 xx들의 각 클래스에 대한 분포가 정규분포를 따를 것으로 

가정하기 때문인데 이에 대해선 추후에 다뤄보도록 하자.

<p align = "center">
  <img width = "400" src = "https://github.com/skdytpq/skdytpq.github.io/blob/master/_pics/logistic_4.png?raw=true">
  <br>
  그림 4.로지스틱함수
</p>



# 훈련과 비용함수

우리는 이제 로지스틱 회귀모델이 어떻게 나왔고 어떻게 해석하는지 살펴봤다. 이제 model을 결정했기 때문에 이 model에 대한 Error을 정의하고 이를 최소화 하는 파라미터를 결정하는 방법에 대해 살펴보자.

우리가 지금 생각하고 있는 문제는 binary classification인데, 출력은 0 혹은 1로 정해져 있다. 즉, 선형회귀 모델과 달리 Input에 대한 결과는 맞거나 틀리거나 둘 중 하나가 된다. 

따라서, 우리는 정답을 맞추었을때는 Error 값을 0으로, 정답을 맞추지 못했을 때는 Error값을 가능한 크게 줄 수 있도록 하자.

​	$P = \frac{1}{1+e^(-(\beta_0 + \beta_1X))}$ 이 식이 우리의 model 출력함수가 되는데 , 우리가 원하는 것은 원래의 라벨 $y$가 $1$일 때 $P$의 값이 0이면 에러 값을 크게 주고, 또, 라벨 $y$가 $0$일 때 $P$​의 값이 1이면 에러 값을 크게 주는 것이다.

이것을 로그함수를 이용하여 쓸 수 있는데, 로그함수의 형태에서는 

$\begin{cases} lim_{x\rightarrow 0^+}\log(x) = -\infty \\ \log(1) = 0\end{cases}$​ 가 되기 때문에 우리는 이 $x$에 $P$를 대입해 생각해본다면 우리의 에러는 

$E(y, P) = \begin{cases}-\log(P) &&\text{ if } y = 1 \\ -\log(1-P) &&\text{  if }y = 0 \\ \end{cases}$  이렇게 생각이 가능하다. 이러한 과정은 나름 직관적으로 이해할 수 있

다. 즉 우리가 모델에 $x$를 input할 때 우리는 $Y$에 대한 확률을 계산한다. 이 과정에서 이 확률이 실제 label의 $y$​값과 다르면 다를수록 이 Error는 기하급수적으로 커질 것이다. 예를들어 실제 label은 0이지만 우리가 예측하는 $P$값이 1과 가까워지면 가까워 질 수록 오차는 $-\infty$로 발산하게 되는 것이다.

각 샘플에 대한 비용함수를 이렇게 정의한다면 우리는 모든 훈련샘플의 비용을 평균하여 전체 훈련세트의 비용함수를 구할 수 있고 이 비용함수는 

$J(\theta) = \frac{1}{m}\sum_{i}^m[y^{(i)}log(\hat{p^{i}})+(1-y^{(i)})log(1-\hat{p^{(i)}})]$​  로 정의되는데 

위 식은 겁먹을 필요없이 그저 어떤 관측치의 label 이 0이라면 $(1-y^{(i)})log(1-\hat{p^{(i)}})$​에서 Error을 구한 후 1이라

면 $y^{(i)}log(\hat{p^{i}})$​​​에서 Error을 구한 후 모든 관측치의 Error의 평균을 구하겠다는 이야기이다.



뜬금없이 위 식에서 $\theta$​​​가 왜 나오는지 이해가 안될 수도 있다. 내가 앞서 $\beta$​​​라고 표현한 이 prameter을 편의상 $\theta$​​​라고 

다시 표현한 것인데 위 비용함수 식에서 확률변수 $P$​​​​​는 $\theta$​(앞서 $\beta$​​​라고 표현한​​)에 따라 달라지기 때문이다. 즉 $P$의 표

현은 원래 $P(X,\theta)$이기 때문에 $\theta$가 튀어나온 것이다.

안타깝게도 이 비용함수의 최솟값을 계산하는 알려진 해가 없어 정규방정식이 따로 없다.하지만 이 함수는 볼록함

수이기 때문에 경사하강법이나 최적화 알고리즘을 통해 전역 최솟값을 찾을 수 있다. 즉 우리는 위 비용함수의 편

도함수를 구한 후 최솟값을 갖는 $\theta$​​를 찾을 수 있다. 여기서 $e$​의 미분은 자기자신이라는 특이한 성질 때문에 편미분

을 계산하는 것이 그렇게 어렵진 않은데 이부분에 있어선 기회가 된다면 다시 작성해 보겠다.



# sklearn을 활용한 Logistic-model 구축

이제 Iris 데이터로 python에서 모델을 직접 만들어 보자.
