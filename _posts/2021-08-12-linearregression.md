---
title: 선형회귀
sidebar:
  nav: docs-ko
toc: true
toc_sticky: true
categories:
  - 기초통계
key: 20210810
tags: 
  -통계학
use_math: true
---

# 어디서 출발하는가

Machine Learning 은 어떤 목적을 가질까? 

<p align = "center">
  <img width = "600" src = "https://github.com/skdytpq/skdytpq.github.io/blob/master/_pics/linear_1.jpeg?raw=true">
  <br>
  그림 1.머신러닝의 접근법
</p>

위 그림은 머신러닝의 기본적인 접근법이다. input과 output을 보고 적당한 모델을 만드는데, 회귀분석도 대표적인 머신러닝의 방법이라고 할 수 있다. 

우리는 이번에 이러한 회귀분석중 한 형태인 선형회귀에 대해서 이야기 할것이다.

예를들어 삶의 만족도와 1인당 GDP의 관계에 대해서 알고 싶을 때, 선형회귀 모델링의 방식은 삶의 만족도와 1인당 GDP의 관계에 선형적인 관계가 있을 것이다라는 가정에서 출발한다.<br>간단한 수식으로 표현한다면 삶의 만족도 = $\theta_0 + \theta_1\times$​​삶의만족도가 되겠다.

여기서 중요한 포인트는 우리가 만든 이 식은 가정이라는 것이다. 즉, 실제 input과 output의 관계가 선형이 아니거나 우리가 만든 선형회귀 직선과 일치하지 않을 수 있지만 우리는 선형이라는 가정을 토대로 과연 이 직선이 유의한지에 대해서 파악한다.

<p align = "center">
  <img width = "400" src = "https://github.com/skdytpq/skdytpq.github.io/blob/master/_pics/linear_2.png?raw=true">
  <br>
  그림 2.기본적인 데이터의 형태
</p>

데이터가 이렇게 나왔다고 가정해보자. 여기서 이 데이터를 보고 가장 간단한 모델을 만들려 할 때 우리는 우선  X와 Y가 선형관계다 라고 가정할 수 있다. 

과연 이 가정은 타당할까? 우선 이 2차원 데이터에 선형회귀 모델을 적합시켜서 직선을 한번 그어보겠다.

<p align = "center">
  <img width = "400" src = "https://github.com/skdytpq/skdytpq.github.io/blob/master/_pics/linear_3.png?raw=true">
  <br>
  그림 3. 가정을 통해 직선을 그었다
</p>

이 그래프는 어떤가? 이 그래프에 나와있는 빨간 직선은 데이터의 추세에 대해서 나름 잘 나타내는 거 같지만 이리저리 튀는 값들이 있다. 사실 우리는 가정에 의해 매우 많은 형태의 직선을 그릴 수 있다. 

다만 그 직선이 저 데이터의 추세를 보여주는 가장 좋은 직선이라고 이야기할 수는 없을 것이다.

우리가 데이터의 관계를 선형이라고 **가정**할 때 그렇다면 가장 최선의 직선은 어떻게 표현될 수 있을까? 또 그 직선은 저 데이터를 얼마나 잘 설명해줄까?

# 비용함수를 정의하자

우리는 이제 데이터의 관계가 선형이라고 한 가정에서 더 확장하여 어떠한 선형관계가 가장 좋은 선형관계인지 즉, 어떤 직선을 그엇을 때 가장 저 데이터의 분포를 잘 설명할 수 있는지에 대해 알아보자.

$Y = \beta_0 + \beta_1x$​​ 를 우리가 찾고자 하는 최적의 직선식이라고 할 때 저 직선에 올라탄 데이터도 있지만 그렇지 않은 데이터도 있기에 데이터와 직선 간의 거리를  오차라고 이야기하고 $\epsilon$​​​이라고 표현할 것이다. 

그렇다면 각 데이터는 $Y_i = \beta_0 + \beta_1x_i + \epsilon_i$​​​​​이라고 표현할 수 있을 것이다. 최적의 직선이면 자연스럽게 여러 직선중 가장 이 $\epsilon$​이 작을 것이다. 

<p align = "center">
  <img width = "400" src = "https://github.com/skdytpq/skdytpq.github.io/blob/master/_pics/lin_4.png?raw=true">
  <br>
  그림 4. 임의의 회귀직선과 오차
</p>

우리가 한 직선을 긋고 그 직선위에 있는 점을 $\hat{y_i}$​​라고 할 때 이 $\hat{y_i}$​​가 실제 관측값인 $y_i$​​와의 차이가 오차 $e_i$​​​​이고 이러한 오차들의 부호를 제거하기 위해 제곱을 취한 후 모두 더해주면<br>$\sum_i^ne_i^2$​​ 다시말해 $\sum_i^n(y_i-\hat{y}_i)^2$​​​​ 이 된다.

즉 이 위의 식(오차제곱합)을 최소화 하는 직선을 찾는다면 우리는 위 데이터가 선형관계일 때 데이터의 추세를 가장 잘 표현해주는 직선을 찾았다고 할 수 있다!

위 식을 우리가 비용함수라고 정의할 때 저 비용함수가 최소가 되게 하는 직선의 식을 찾기 위해 해석적인 방법을 적용할 수 있다. 

다시말해 바로 결과를 얻을 수 있는 수학 공식이 있는데, 이를 정규방정식이라 한다.

즉, 만약 데이터가 2차원이라면 우리는 $\hat{y_i}$ 를 $\beta_0 + \beta_1x$라고 표현할 수 있을 탠데 그렇다면 오차제곱합의 형태는 prameter가 2개인

 $\sum_i^n(y_i-(\beta_0+\beta_1x))^2$​​​의 ​형태가 된다. 즉, $\beta_0,\beta_1$​​이라는 변수가 2개인 이 식에서 편미분을 통해 직접 정규방정식을 어렵지 않게 구할 수 있다.

# 2차원이 아닌 경우엔?

하지만 보통의 데이터를 살펴보면 이렇게 2차원의 형태가 아닌 받는 Parameter가 훨씬 많은 경우가 많다. 따라서 방정식의 형태가 아닌 행렬식으로 $Y$​와$X$​​​를 나타내는데 기본적인 가정과 흐름은 변수가 2개인 형태와 다르지않다.

2차원이 아닌경우에 선형회귀에서의 가정이 시각적으로 직관적이지 않을 수도 있지만 $X$가 p차원의 공간이라고 할 때 $X$와$Y$​​​​가 p차원 공간에서 선형관계를 갖는다 라고 간단하게 해석할 수 있다. 

이 p차원 공간에서 이 직선이 어떤 방향으로 어떤 형태로 그려지는지 결정해주는 게 parameter라고 이해할 수 있다. 

사실 이러한 해석은 비교적 정확하지 않고 각각의 데이터 열(column)벡터인 $\vec{x_i}$​​로 생성되는 열공간에 가장 가깝게 사영된 $\vec{y}$​​를 구하여 적절한 $\vec{x_i}$​​​를 구하는 과정으로 생각할 수 있는데 이러한 내용은 선형대수학적 내용이라 더 공부한 후 포스팅해보겠다.

<p align = "center">
  <img width = "400" src = "https://github.com/skdytpq/skdytpq.github.io/blob/master/_pics/lin%205.png?raw=true">
  <br>
  그림 5.A의 열공간에 포함되지 않은 벡터 b
</p>

위 그림과 같이 열공간에 포함되지 않은 $\vec{b}$를 열공간 $Col(A)$​로 가장 가깝게 사영시킨 $\vec{p}$를 계산하여 $\vec{a_1}$과 $\vec{a_2}$의 선형조합을 구하는 과정이다

아무튼 변수가 2개 이상인 선형회귀 식을 행렬로 간단히 $ \hat{y} = \theta \times x$​​로 표현할 수 있고 여기서 $X$​​는 $n\times p$​​ 의 행렬이며 여기서 $n$​​은 관측데이터의 수이고$p$​​는 parameter의 수에 intercept인 $\beta_0$​​​를 더한 것이다. 

이런 중선형회귀 식에서도 정규방정식을 구할 수 있는데 행렬로 간단히 표현이 가능하며 식은 $\hat{\theta} = (X^TX)^{-1}X^Ty$​​​​​가 된다.

우리는 이렇게 정규방정식을 통해 데이터의 추세를 선형적으로 가장 잘 설명해줄 수 있는 직선을 구할 수 있다! 

이제 우리는 데이터를 줘서 모델에게 직선을 구하게 한 후 예측을 하고 그 예측이 얼마나 정확한지 알 수 있다. 

하지만 이런 정규방정식을 컴퓨터에서 계산하기엔 컴퓨터가 너무 아프다.. 미분 후 나온 연립 방정식으로 각각의 $\theta$를 구하게 되는데, 실제 우리가 machine learning에서 사용하는 데이터는 parameter도 많고 데이터 수도 많기때문에 계산량이 어마어마하다.

실제 $X^TX$​의 역행렬을 계싼하는 계산 복잡도(computational complexity)는 특성이 두배가 되면  $2^{2.4}$~$2^3$​​​배 사이로 증가하기때문에 특성 수가 많으면 매우 느려진다.

그렇기에 우리는 훈련샘플과 특성이 많을 때 적합한 훈련방법인 경사하강법에 대해 알아보자

# Gradient Descent(경사하강법)

앞서 우리가 정의한 $\sum_i^n(y_i-\hat{y}_i)^2$​ 은 비용함수(cost function)라고 부르기도 하는데, ​모델을 구하는 과정에서 이 비용함수가 작을수록 데이터에 대한 설명능력이 좋다고 할 수 있다.

이러한 경사하강법의 기본 아이디어는 비용함수를 최소화하기 위해 반복해서 파라미터를 조정해가는 것이다.

> 짙은 안개 때문에 산속에서 길을 잃었다. 나는 발밑 지면의 기울기만 느낄 수 있다. 빨리 이 골짜기를 벗어나고 싶기에 나는 한발 한발 모든 방향에서 가장 가파른 길을 따라 내려간다.

이 문구는 경사하강법의 개념을 이야기 할 때 많이 인용되는 문구이다. 

구체적으로 임의의 $\theta$​​에 대해서 한번에 조금씩 저 비용함수가 감소되는 방향으로 진행하여 알고리즘이 최솟값에 수렴할 때까지 점진적으로 향상시킨다.

아래의 그림에서 볼 수 있듯, gradient의 방향은 함수값이 커지는 방향이다. 다시말해 gradient는 기울기가 **커지는**방향으로 정의된다. 

<p align = "center">
  <img width = "400" src = "https://github.com/skdytpq/skdytpq.github.io/blob/master/_pics/lin6.png?raw=true">
  <br>
  그림 6.정의역이 
a
 와 
b
 (여기선 slope, intercept)이고 높이가 비용함수의 값인 함수 공간에서 임의의 포인트에서의 gradient 방향은 함수값이 커지는 방향이다.
</p>

이 그림은 parameter가 a와 b로 정의된 후 높이가 비용함수 인데 좌측하단, 우측상단으로 갈수록 높이가 높아지는 즉, 비용함수가 커지는 3차원 공간으로 해석하면 보다 더 직관적일 수 있다. ($x$축은 선형회귀식의 기울기(slope), $y$축은 intercept($y$​​ 절편), z축은 비용함수!)

따라서 gradient의 방향은 함수가 ‘커지는’ 방향이므로 우리는 이 반대 방향으로 한 스텝, 한 스텝 $a$와 $b$의 위치를 업데이트 해간다면 결국은 비용함수$E  =f(a,b)$의 최소값(별표) 위치까지 $a$,$ b$​를 옮겨갈 수 있을 것이다

<p align = "center">
  <img width = "400" src = "https://github.com/skdytpq/skdytpq.github.io/blob/master/_pics/lin%207.png?raw=true">
  <br>
   그림 7.정의역이 a 와 b (여기선 slope, intercept)이고 높이가 비용함수의 값인 함수 공간에서 임의의 포인트에서 gradient의 반대방향으로 a와 b의 위치를 업데이트 해간다면 결국 비용함수가 최소가 되는 a와 b를 찾아갈 수 있을 것이다.
</p>

결과과적으로 우리가 구하고자 하는 함수 $f(a,b)$에서 파라미터$(a,b)$를 임의의 값으로 설정한 뒤 업데이트를 할 수 있다. 다시말해 랜덤하게 배정된 포인트에서 증가하는 방향의 반대방향으로 가다보면 비용함수의 값이 최소가 되는 지점에 도달할 수 있다.

즉, 벡터 $[a, b]^T$​의 식에 대해 $\begin{bmatrix}a\newline b \end{bmatrix}:=\begin{bmatrix}a\newline b\end{bmatrix} -\alpha\nabla f(a, b)$​​ 이렇게 업데이트를 ​해줄 수 있다. 여기서 $\alpha$는 learning rate인데 이 부분은 다음 게시물에 좀 더 자세히 이야기해보겠다.

