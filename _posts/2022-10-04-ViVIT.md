---
title: ViVIT
sidebar:
  nav: docs-ko
toc: true
toc_sticky: true
categories:
  - 딥러닝
key: 20221004
tags: 
  - 딥러닝
use_math: true
---

# ViVIT: A Video Vision Transformer

## Introduction

시작하기에 앞서, Video 의 처리는 Image 에서 Tranformer에 접근한 것 보다 더욱 복잡한 작업이 될 수 있다. Image 의 경우 우리가 생각할 수 있는 Inductive bias는 Spatial한 특성뿐이라고 할 수 있는데, Video 에서는 Sequence에 대한  가정도 추가되기 때문에 어찌보면 Image 보다 Transformer에 더 잘 어울릴 수 있는 Task라고 할 수 있지만, 이러한 Sequence를 Transformer의 Encoder로 처리하는 것은 정보를 바라보는 관점이 두 가지이기 때문에 보다 더 복잡한 Task 라고 할 수 있다.

VIT에 영향을 받아, Transformer based- video classification 모델을 개발하는 것을 설계하였다. 최근 Video 를 다루는 Performant한 모델의 구조는 대부분 3D Convolution 을 사용하기 때문에 self-attention 이전 3D Convolution을 적용한 뒤, Self attention 을 통해 Long range dependencies를 극복하고자 한다.

<p align = "center">
  <img width = "600" src = "https://github.com/skdytpq/skdytpq.github.io/blob/master/_pics/ViVIT/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202022-10-04%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%208.20.14.png?raw=true">
  <br>
  그림 
</p>

본 논문의 저자는 해당 그림과 같은 아키텍쳐를 제시하였다. 가장 메인이 되는 점은 물론 Self-attention을 통해 global information 을 취득한다는 점이다. 또한 해당 Self attention 을 위한 구성 요소들은 각각 spatio - temporal token의 sequence로 구성되어있다.

Video에서는 시공간 Token이 매우 많이 나오게 되는데, 이러한 토큰을 효율적으로 처리하기 위해 시간과 공간 차원에서 model 의 factorising을 진행하여 scaling도 보존하고자 하였다. 또한 기존 VIT처럼 매우 방대한 양의 데이터가 아닌 비교적 적은 데이터에서의 학습을 가능하게 하기 위해 pretrain된 image model을 leveraging하는 것과 모델을 정규화 하는 것을 보인다.

또한 Pure Transformer 모델에서 어떻게 최적의 토큰화 방법, 정규화 방법을 찾는지에 대해 ablation analysis 를 통해 제시할 것이다.

## Video Vision Transformers

### Overview of Vision Transformer

우선 2D Image에 대해 설명한 Vision Transformer에 대해서 간략하게 살펴보자면, N 개의 non-overapping된 Patches로 구성되어 있으며 각 Patch $x_i \in R^{h \times w}$ 는 Linear Projection 을 통과한 이후, $z_i \in R^{d}$로 위치하게 된다.



-  $z = [z_{cls},Ex_1,Ex_2 , \dots ,Ex_N] + p$
  - 해당 식에서 $E$는 Patch를 구성할 때의 Convolution이라고 생각할 수 있으며 $p$ 는 position embedding을 의미하는데, 해당 position embedding 은 학습된 임베딩이다. 

해당 Vector는 Layer Normalization 과 MSA를 거친 후 MLP를 총 $L$번 거치게 되며 $z^L_{cls} \in R^d$ token을 최종 output으로 내놓게 된다.

### Embedding video clip

본 논문의 저자는 앞선 ViT의 개념을 차용하면서도 sequence token과 video 를 mapping하기 위해 두가지 간단한 방법을 제안하였다. 그 이후 positional embedding을 거친 후 $R^{N \times d}$ 공간의 Vector $z$를 얻게 된다.

#### Uniform frame sampling 

<p align = "center">
  <img width = "600" src = "https://github.com/skdytpq/skdytpq.github.io/blob/master/_pics/ViVIT/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202022-10-04%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%208.51.29.png?raw=true">
  <br>
  그림 
</p>

해당 그림과 같이 Video tokenization의 가장 직관적인 방법은 Videp clip 에서 $n_t$개의 샘플을 균일하게 뽑는 것이다. 이 과정에서 ViT와 같이 모든 image 들의 embedding은 독립적으로 진행된다. 이후 tokens를 함께 붙이게 된다. 만약 이 과정에서 각 frame에서 $n_h \cdot n_w$의  image patch가 뽑히게 된다면, 결과적으로 $n_t \cdot n_h \cdot n_w$ 의 토큰이 트랜스포머 인코더에 전달되게 된다.

직관적으로 해당 과정의 경우 어찌보면 비디오 시퀀스를 하나의 거대한 2D image취급을 하게 하는 것이다. 결과적으로 어떠한 temporal 한 특성을 지정해 주는 것이 아닌, 여러개의 patch를 이어 붙이게 되어 concurrent work에 의해 사용되는 입력 임베딩 방법이라 할 수 있다.

#### Tublet embedding

<p align = "center">
  <img width = "600" src = "https://github.com/skdytpq/skdytpq.github.io/blob/master/_pics/ViVIT/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202022-10-04%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%208.51.47.png?raw=true">
  <br>
  그림 
</p>

위 방법의 대체 방법으로는 해당 그림과 같이 spatio-temporal 한 일종의 ‘tube’를 input volume에서 추출하는 방식이다. 이후 이 token을 $R^{d}$차원에 projection하여 한 sequence token으로 넣게 된다. 이 method는 ViT의 embedding 을 3차원으로 확장시킨 것이라고 할 수 있다.

이 Tublet의 dimension 은 $t \times h \times w$ 이며 $n_t = [\frac{T}{t}], n_h = [\frac{H}{h}],n_w = [\frac{W}{w}]$ 이다. 각 차원은 동일한 Spotial 에 대해 temporal 차원을 추가한 것이다. 이 Tublet 의 크기는 커질수록 컴퓨터 연산량이 증가한다. 직관적으로 해당 과정은 시공간 정보를 융합하는 토큰화라고 할 수 있으며 Uniform frame sampling보다 시간적 정보를 더 사용한다.

### Transformer Models for Video

앞서 제시한 전체적인 아키텍쳐에서 본 논문은 중첩된 트랜스포머 구조를 제안하였다.

#### model1 : Spatio temporal attention

해당 방법의 경우 Video로부터 추출한 모든 Spatio temporal attention을 transformer encoder에 입력하는 것이다. layer의 개수에 따라 receptive field가 선형적으로 증가하는 cnn network와 다르게 각 레이어는 모든 토큰 쌍의 상호작용을 모델링하기 때문에 복잡도가 급수적으로 증가하게 된다. 즉, input frame이 많아질수록 계산 복잡도가 매우 커지게 된다.

####  Model 2: Factorised encoder

<p align = "center">
  <img width = "600" src = "https://github.com/skdytpq/skdytpq.github.io/blob/master/_pics/ViVIT/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202022-10-04%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%209.48.47.png?raw=true">
  <br>
  그림 
</p>

해당 모델은 위 그림처럼 분리된 두 개의 인코더를 지니고 있다. 모델에서의 유일한 상호작용은 동일한 idx를 지닌 temporal token이다. 즉, 다시말해 동일한 시간에서의 image를 우선적으로 spatial transformer encoder에 통과시킨 후 이후 다른 시간의 token 간의 유사도를 구하는 것이다. 각 temporal index는 $h_i \in R^{d}$ 로 되어있으며 Transformer encoder는 $L_s$ 개의 layer로 구성되어 있다. 

이후 Encoder를 통과하여 encoded된 classification token $z_{cls}^{L_s}$을  구하게 된다. 이후 이 임베딩된 토큰은 상위 계층에서 Temporal Transformer Encoder로 들어가게 된다. 각 frame-level 의 representation인 $h_i$ 가 컨캣되어 $H \in R^{n_t \times d}$ 의 벡터를 인코더의 input으로 집어넣게 되는데, 이 과정에서는 각 시간 간의 interaction 을 구한 후 최종적으로 나오는 Token이 전체적인 Classfy를 위한 Token이 된다.

즉, 다시말해 하위 Encoder의 결과 Token을 상위 Encoder의 Input으로 집어넣어 각 시간 공간 차원을 독립적으로 학습한다는 의미라고 할 수 있다.

해당 아키텍쳐는 temporal information의 late fusion 과 동일하다고 할 수 있으며 Layer의 수가 많음에도 불구하고 계산 복잡도는 오히려 이전 모델보다 감소한다.

#### Model 3: Factorised self-attention

<p align = "center">
  <img width = "600" src = "https://github.com/skdytpq/skdytpq.github.io/blob/master/_pics/ViVIT/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202022-10-04%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%2010.03.07.png?raw=true">
  <br>
  그림 
</p>

해당 모델의 경우 layer의 수가 Model 1과 동일하나, 위 그림과 같이 모든 토큰 쌍에($z^{l}$) 대해 Multi-head Attention을 적용하는 것이 아니라 하나의 큰 Trasnformer Encoder 안에 두 개의 직렬적 Encoder가 존재하는 구조로 되어있다. 순차적 Encoder 구조에서 최초 Encoder는 model 2와  마찬가지로 공간 정보에 대해 attention 을 진행하며 이후 시간 정보에 대한 attention을 진행한다. 

이 과정은 Model 1과 Layer의 수가 같으면서도 Model 2 만큼의 시간 복잡도를 지니게 되는데, 한번에 쌍으로 연산하는 것이 아닌, 각 Encoder마다 쌍중 하나의 정보를 사용하여 attention을 진행하기 때문이다. 

본 논문에서는 해당 Attention 과정에서 공간 정보를 먼저 학습시키는지, 시간 정보를 먼저 학습시키는 지에 대해 차이점을 발견하지 못했다고 한다.  해당 과정을 진행할 때의 Token $z$ 는 $z \in R^{n_t \times n_h \cdot n_w \cdot d}$ 차원에 존재하게 된다.

#### Model 4 : Facotorised dot product attention 

<p align = "center">
  <img width = "600" src = "https://github.com/skdytpq/skdytpq.github.io/blob/master/_pics/ViVIT/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202022-10-04%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%2010.19.08.png?raw=true">
  <br>
  그림 
</p>

최종적으로 Model 2 와 Model 3과 같은 계산 복잡도를 가지며 Model 1과 같은 양의 파라미터(풍부한 학습)을 갖는 Model 을 소개한다. 시공간의 차원을 분리하는 것은 Model 3의 느낌과 비슷하지만, 본 논문의 해당 모델의 경우 직렬적인 순서를 갖는 operation을 진행하는 것이 아닌, 위 그림과 같이 각자 다른 head 를 갖는 attention을 진행하게 된다.

좀 더 구체적으로 말하자면 본 논문에서는 각 token에 대해서 시공간 정보를 분리하여 weight를 주고 다른 head에서 attention을 진행하게 된다.

기본적으로 어텐션 수식은 

$Attention(Q,K,V) = Softmax(\frac{QK^T}{\sqrt{d_k}}V)$ 의 형태가 되는데, 여기서의 $Q,K,V$  와 해당 input인 $X$는 $\R^{N\times d}$ 영역에 속하게 된다. 여기서 $N=n_t \cdot n_h \cdot n_w$  이다. 여기서의 Main idea는 query 에 대한 각 Key, Value가 동일한 Spatial-Temporal index를 갖는 token에 집중하게 한다는 것이다. 

이 것을 가능하게 하기 위해 $K_s , V_s \in \R^{n_h \cdot n_w \times d}$ , $K_t,V_t \in \R^{n_t \times d}$  영역에서 진행된다. 즉, Temporal 공간에서는 동일한 공간을 고정시켜 놓은 뒤 시간 변화에 대해서만 Attention 을 진행하며, Spatial 영역에서는 시간을 고정시켜 놓은 뒤 공간들의 변화에 대해 전체적인 Attention을 수행하는 것이다.

이렇게 된다면 전체 Attention 의 결과값 $Y_s = Attention(Q,K_s,V_s) , Y_t = Attention(Q,K_t,V_t)$ 가 생성되게 된다.

해당 과정에서 Query 의 이웃만 바꾼 것이기 때문에 $Y_s,Y_t$의 차원은 동일하다. 따라서 Attention 과정을 마친 후 최종 output $Y = concat(Y_s,Y_t)W_O$ 가 나오게 된다.

### Initialization by leveraging pretrained models

ViT의 경우에는 매우 큰 데이터 셋으로 pre-trained 되었을 때에 더욱 효과적인 것을 입증하였다. 또한 그 이유는 Inductive bias가 없기 때문이라고 할 수 있다. 하지만 Video의 경우 이러한 방대한 데이터 셋을 구하기가 쉽지 않다.따라서 큰 모델을 바로 학습시켜 높은 성능을 내는 것은 매우 어려운 과제이다. 이러한 문제를 해소하기 위해 본 논문은 image model의 weight로 video 모델의 weight를 초기화 하는 방향을 설정하였다.

하지만 이러한 방식은 존재하지 않는 매개 변수들의 초기화 방법을 이야기 할 때 적절하지 않을 수 있다. 따라서 본 논문은 두 가지의 효과적인 전략을 제시한다.

#### Positional embeddings

image model의 경우 토큰이 input마다 존재하게 된다. 그러나 video의 경우 temporal 축이 추가로 존재하기 때문에 $n_t$ 배 만큼의 토큰이 추가적으로 필요하다. 결과적으로 본 논문은 해당 경우를 동일 공간의 index를 가진 tube 마다 반복해서 동일한 token으로 초기화를 진행하며 이 동일한 embedding 결과는 미세 조정된다.

#### Embedding weights, E

Tabelet embedding 토큰화 임베딩을 진행할 때 $E$ 는 3D tensor이다.  사전 학습 된 2D tensor $E_{image}$ 를 3D 공간에 단순하게 inflate 하는 방법이 존재할 수 있다.($E = \frac{1}{t}[E_{image1} , \dots , E_{image}]$)

하지만 본 논문에서는 추가적인 전략을 고안하였는데, 전체 Frame에 중앙에 해당하는 곳에는 $E_{image}$ 벡터를 할당하고 나머지 시간 영역에 대해서는 0을 부여하는 방향을 취한다.

이 방법을 사용하면 3D conv filter는 initialisation에서 'Uniform frame sampling'과 동일하게 작용하고 이와 동시에 시간 정보도 학습 가능하다.

<p align = "center">
  <img width = "600" src = "https://github.com/skdytpq/skdytpq.github.io/blob/master/_pics/ViVIT/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202022-10-04%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%2011.22.39.png?raw=true">
  <br>
  그림 
</p>

해당 그림의 결과와 같이 이 방법은 central frame을 사용하였을 때의 성능의 향상을 볼 수 있어 본 논문의 실험은 지속적으로 해당 방법론을 사용하였다.

## Experiments

ViT-Base $(ViT-B, L=12, NH =12, d=768)$, ViT-Large $(ViT-L, L=24, NH=16, d=1024)$,
ViT-Huge $(ViT-H, L=32, NH =16, d=1280)$

해당 논문의 Param이다.

해당 논문은 여러가지 실험을 진행하였다.

<p align = "center">
  <img width = "600" src = "https://github.com/skdytpq/skdytpq.github.io/blob/master/_pics/ViVIT/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202022-10-04%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%2011.27.18.png?raw=true">
  <br>
  그림 
</p>

아래의 그림을 보면 model 1의 설계가 좀 더 성능이 높음을 확인할 수 있다. 하지만 Runtime 측면에서는 역시 계산 복잡도의 영향 때문에 비교적 오래 걸리는 것을 확인할 수 있다.

<p align = "center">
  <img width = "600" src = "https://github.com/skdytpq/skdytpq.github.io/blob/master/_pics/ViVIT/%E1%84%89%E1%85%B3%E1%84%8F%E1%85%B3%E1%84%85%E1%85%B5%E1%86%AB%E1%84%89%E1%85%A3%E1%86%BA%202022-10-04%20%E1%84%8B%E1%85%A9%E1%84%92%E1%85%AE%2011.29.56.png?raw=true">
  <br>
  그림 
</p>

위에서 시행된 실험에서는 input frames을 32로 고정 했었지만 변화하며 이를 실험해보았다. input frames의 수를 늘려가며 token의 개수를 일정하게 하기 위해 tubelet length t를 조절했다. 위의 표에서 알 수 있듯이 일정 수준에서는 model이 input video clip을 모두 볼 수 있어서 정확도가 saturate했다. **동영상 길이가 다를 경우에 이 결과를 참고**해야할듯하다.

또한, **해당 논문의 모델은 토큰의 개수를 늘릴 필요  없이 더 긴 영상을 처리**할 수 있다는 것을 알 수 있다
