---
title: HIFI-GAN
sidebar:
  nav: docs-ko
toc: true
toc_sticky: true
categories:
  - 딥러닝
key: 20221220
tags: 
  - 딥러닝
use_math: true

---

# HIFI-GAN

## Introduction

대부분의 음성합성에 관한 신경망 학습 모델들에서는 크게 두 단계의 Pipe line 이 존재한다.

- Mel Spectrogram or linguistic feature 라는 저 해상도의 중간단계의 표현을 예측하는 것 (Mel 은 결과적으로 Vocoder를 통과하기 위한 중간단계의 output 이라고 판단한다)
- 중간 단계의 representation 을 통해 raw waveform 추출과정

첫번째 단계는 저차원 공간의 표현을 찾는 단계라면 두번째 단계는 초당 24000번 이상의 sampling , 16비트 이상의 raw waveform 을 합성하는 단계이다. 

본 논문에서는 mel을 통해 두번째 단계의 output 인 raw waveform 을 보다 높은 fidelity 를 갖게 디자인하고자 하는 목적이 있다. 

선행 연구에서 수행한 대표적 Vocoder인  Wavenet의 경우 auto-regressive(AR) 기반 신경만 모델이며 해당 모델은 신경망 음성 합성의 질을 높였지만 해당 모델은 매우 느리다는 단점이 있다.

그 이유는 초당 24000번 이상 sampling 을 진행해야 하는 2단계 과정의 경우 AR 모델은 이전 sample의 정보를 토대로 계산되어야 하기 때문에 계산량이 매우 많아 느리게 계산되기 때문이다. 

생성 분야에서 지배적인 지위에 있는 GAN 의 경우 음성 합성 분야에도 적용되는 시도가 있었다. 

Mel 기반의 생성 모델인 MelGAN, ClariNet, linguistic features 기반의 생성 모델인 GAN-TTS 등이 있다.

GAN 기반 모델의 경우 AR 혹은 Flowbased 기반 모델보다 낮은 계산량과 속도를 내고있지만 sample의 quality의 경우 비교적 gap 이 존재한다.

HIFI-GAN의 경우 AR기반 모델에 비해서도 waveform의 quality가 좋으며 기존 GAN 기반 모델보다 효율적인 컴퓨팅 파워를 낼 수 있는 모델이다.

음성의 경우 여러 정현파들의 신호가 다양한 period 에 걸쳐 이루어져 있다. 해당 과정에서 이러한 각 정현파 신호들이 언제 얼마만큼의 영향력을 가지는지에 대한 periodic pattern을 모델링하는 것이 중요하다고 할 수 있다.

**따라서 해당 모델은 discriminator 를 sub discriminator로 나누어 각 sub 들이 raw waveform의 특정 구간 정보만을 얻도록 설계하였다.**

이러한 구조는 HIFI-GAN 성능의 지대한 영향을 끼쳤다고 논문의 저자는 밝힌다.

또한 discriminator가 audio 의 각기 다른 부분을 추출함에 따라 다중의 residual block 으로 되어있는 module을 배치하여 다양한 길이의 pattern 을 parallel 하게 파악할 수 있게 하였으며 이 것을 generator에 적용하였다.

## HIFI-GAN

HIFI-GAN은 하나의 generator 와 두 개의 discriminator(multi-scale , multi-period)로 이루어져 있다. 

### Generator

<p align = "center">
  <img width = "500" src = "https://github.com/skdytpq/skdytpq.github.io/blob/master/_pics/pics/hifi_gan1.png?raw=True">
  <br>
  그림 
</p>

Generator은 기본적으로 fully convolutional network로 구성되어있는데, mel을 input 으로 넣어 output sequence 가 raw waveform 과 동일해질때까지 upsampling 을 진행한다. 

이 과정에서는 기존에 익숙한 convolution 이 아닌, transposed convolution 을 사용하는데 이 과정의 목적 의식은 input 을 받고 featuremap 이 해당 input 의 공간적 정보를 갖고 있으며 size를 크게하고자 할 때 진행하게 된다.

기본적으로 raw waveform 의 길이는 time domain 영역에서 mel 보다 길기때문에 이러한 Convolution 방법을 사용한다고 판단된다.

#### Transposed Convolution

<p align = "center">
  <img width = "500" src = "https://github.com/skdytpq/skdytpq.github.io/blob/master/_pics/pics/hifigan_2.png?raw=True">
  <br>
  그림 
</p>

Transposed convolution의 경우 다음 그림과 같이 진행되게 되는데, 단순하게 기존 Convolution 의 역순이라고 생각할 수 있다.

단순하게 Input 의 정보를 받아 3X3 kernel에 곱해서 output의 대응하는 자리에 집어넣으면서 진행되는데, 빗금친 부분처럼 겹치는 부분은 단순하게 해당 지역의 원소를 더해주게 된다.

해당 과정은 Deconvolution을 통해 feature map 을 구현하는 것이 목적이 아니기 때문에 Convolution 의 역연산이 아니라 Upsampling의 목적을 띄고있어 명확한 차이가 존재한다. (Kernel 을 그대로 쓰는지, update 와 학습의 대상인지)

#### MRF(Multi-Receptive Field Fusion)

Generator는 앞선 모든 Transposed Convolution을 진행한 뒤  MRF 모듈을 따르게 된다.

해당 모듈은 다양한 길이의 패턴을 parallel하게 관측하게 되는데, 세부적으로 MRF 모듈은 그 내부에 있는 residual block들의 output을 모두 더한 뒤 반환하게 된다.

여기서 Kernel size, dilation rate 값들은 각 residual block 마다 다르게 설정되게 되며, 이 과정은 receptive field의 패턴을 보다 다양하게 해준다.

해당 MRF 에서는 hidden-dimension($h_n$) ,  kernel_size of tranposed convolutions($k_u$) ,  kernel size($k_u$), dilation rate($D_r$) 의 parameter 가 존재하며 해당 모듈의 호율성과 sample 들의 quality 는 trade-off 관계에 있다.

#### result_generator

전반적인 과정은 다시 정리하자면 transposed conv, MRF 두 단계로 나뉘게 되는데 이러한 과정을 진행하는 이유는 다양한 period 의 정보를 얻고자 함에 있다.

우선적으로 진행하는 transposed conv 의 경우 mel 을 waveform 의 sequence 와 맞춰주는 upsampling 의 목적성을 갖고있으며, MRF 의 경우 해당 upsampled 된 각 sample 들을 다양한 kernel 들로 재구성 함으로써 다양한 preiod 페턴 feature 를 얻게된다

