---
title: 베이지안 서론
sidebar:
  nav: docs-ko
toc: true
toc_sticky: true
categories:
  - 딥러닝
key: 20230129
tags: 
  - 
use_math: true
---

# Bayesian 서론

## 조건부 확률과 표본공간



$\begin{align}
 P( X|A ) &=   \frac{P(X\cap A)}{P(A)} 
\end{align}$ : 조건부 확률의 식 

 조건부 확률의 기본적인 의미는 다음과 같다.

- A라는 사전이 일어났을 때, X 라는 사건이 일어날 확률은?

이러한 의미는 식을 보면 좀 더 직관적으로 이해가 가능하다. 

$\frac{P(X\cap A)}{P(A)} $해당 식에서 분모의 $P(A)$ 는 $A$라는 사건이 일어날 확률로 전체 표본 공간 $S$ 의 분할이라 표현할 수 있다.

여기서 $S$ 라는 표본공간의 의미는 사건이 일어날 공간 이라고 생각하면 된다. 즉, 다시 말해 어떤 확률의 표현을 집합의 개념으로 설명하겠다는 것인데, 모든 사건들이 일어날 공간을 합친 것이 표본 공간 $S$이다.

- 3개 공장에서 제품을 생산하는데, 각 공장 1일 생산량은 A공장 100단위, B공장 50단위, C공장 80단위 이다. 1일 생산량 중 평균적으로 A공장에서 3개, B공장에서 1개, C공장에서 2개의 불량제품이 생산되었다.

다음 예시를 보자.

전체 공장에서 하루동안 생상되는 총량은 100 + 50 + 80 = 230개가 된다. 그렇다면 전체 공장에서의 불량률은 $\frac{3+1+2}{230}$ 이 될 것이다.

A 공장의 불량률은 어떻게 될 것인가 ? A 공장은 총 100개 중 3개의 불량품이 생겨나므로 $\frac{3}{100}$ 의 불량률을 지닐 것이다.

이 예시에서는 제품 생산의 공정의 루트를 총 3가지로 지정하였다. A 공장 , B 공장, C 공장. 즉, 이 3 개의 공장에서 생성되는 제품 외에 다른 외부 요인은 해당 예시에서 들어올 수 없게 된다.

앞서 말한 표본공간 $S$ 란 이렇게 예시에서 가정할 수 있는 모든 요인들을 합친 경우, 가령 앞선 예시에서의 오직 3 개의 공장이라고 얘기할 수 있다. 

그렇다면 저 표본공간 $S$ 에서 일어나는 요인은 $P(A),P(B),P(C)$ 이렇게 세 요인으로 분류할 수 있게 된다. 

이렇게 표본공간에서 분리될 수 있는 각기 다른 요인들을 사상이라고 이야기 하게 된다.

다시 말해 표본공간 전체에 대한 확률 $P(S) = P(A) U P(B ) U P(C)$ 가 된다.

## 베이즈 정리

사상 $A_1 , A_2 , \dots , A_n$ 이 표본 공간 $S $의 분할이며 $P(A_i) >0 ,P(X)>0$ 이라면 다음의 관계식이 성립한다.

$ P(A_k|X) = \frac{P(A_k \cap X)}{P(X)} = \frac{P(A_k) P(X|A_k)}{\sum_{i=1}^n P(A_i) P(X|A_i)}  \propto P(A_k) P(X|A_k) $

우선  전체 $U_{i=1}^n A_i = S$ 가 되는 각 $A_i$ 를 표본공간 $S$ 의 분할이라고 이야기한다. 여기서 각 $A_i$ 간의 교집합은 공집합으로 가정한다.

$\sum_{i=1}^n P(A_i) P(X|A_i)$ 식의 부분을 살펴보자.  

해당식을 풀어 전개하게 된다면, $\sum_{i=1}^n P(X \cap A_i)$ 가 된다. 직관적으로 해석하자면 전체 분할 사상과 사건 $X $ 가 동시에 일어날 확률을 의미한다.

그러나 해당 식은 Deterministic 한 식이기 때문에 특정 상수 $C$로 취급할 수 있게 된다.

그렇게 된다면 $P(A_k|X) = C\times P(A_k \cap X) \propto P(A_k) P(X|A_k)$라고 취급할 수 있게 된다.

해당 식의 의미는 어떻게 해석할 수 있는가?

다음 Example 을 확인해보자 

**A매장에서는 x제품과 y제품의 매출비중이 6:4이며, B매장에서는 x제품과 y제품의 매출비중이 3:7이라 한다. A매장과 B매장의 매출비중은 4:6이다. 임의의 x제품이 A매장의 물건일 확률은?**

위 예시에서 우리가 알고싶은 것은 만약 관찰한 제품이 x 제품일 때 이 것이 A 매장의 물건일 확률을 구하는 것이다.

A 매장에서 제품이 있을 때 해당 제품이 x 일 확률을 구하는 것 즉,우리에게 익숙한 조건부 확률의 경우는 해당 관점과 반대되는 관점이다. 

우리는 베이즈 정리를 통해,  $P(x \vert A)$ 와 같은 사전 지식이 있다면, 반대 관점에서의 새로운 지식을 획득할 수 있게 된다.

- $P(A_k)$: $A_k$의 사전 확률 (prior probability)
- $P(A_k|X)$: X가 주어질 때 $A_k$의 사후확률 (posterior probability)
- $P(X|A_k)$: $A_k$가 주어질 때 X의 우도 (likelihood)
- $P(X)$: X가 발생할 확률 - normalizing constant

 해당 용어는 위에 있는 베이즈 정리 식에서 각 부분에 대한 용어이다. 

사전 확률이란 말 그대로 사전에 어떠한 믿음을 갖고 이러한 분포를 따를 것이다 가정하는 것이며, 우도란 해당 사전 분포가 주어졌을 때 어떤 사건 $X$ 가 발생할 확률을 말하게 된다. 

MLE(maximum likelihood) 추정법이란 앞서 이야기한 사전 분포가 주어졌을 때 해당 사전 분포에서 $X$가 발생할 확률이 가장 높은 지점에서의 $\theta$ 를 찾는 것이라고 간단하게 생각할 수 있다.

앞선 베이즈 정리의 식을 확률 밀도 함수의 표현으로 나타낸다면 다음과 같다.

$$ p(\theta|y) = \frac{p(y|\theta)\pi(\theta)}{p(y)} \propto p(y|\theta)\pi(\theta) $$

- $\theta$: frequentist 방법에서 주로 추정의 대상이 되는 모수 
    - 정규분포의 $\mu, \sigma^2$, 이항분포의 $p$ 등 
- $y$: 관측된 데이터 
- $\pi(\theta)$: $\theta$의 사전분포
- $p(y|\theta)$: y의 우도함수 
- $p(\theta|y)$: $\theta$의 사후분포  
- 베이지안의 해석은 사전에 일어난 사건이 사후 확률에 영향을 얼마나 미치며, 그 사후 확률이 어떻게 되는가에 대한 지표이다.
- 데이터가 주어졌을 때 데이터의 분포를 통해 우리가 알고자하는 확률의 근사를 알 수 있다.
$\theta$ 샘플링을 통해 각 $\pi(\theta),p(y|\theta)$ 를 구하는데, 이 것은 해당 $\theta$ 분포에서 각 값과 각 $\theta$를 통해 구한 $y$ 값의 확률을 계산한 후 곱하는 과정

## 베이즈 정리 - 동전의 앞뒷면

베이즈 정리의 아이디어를 이해하기 위해 동전 던지기의 상황을 한번 가정해보자

- 동전의 앞면이 나올 확률 : $\theta$ 
  - 우리는 해당 $\theta$ 가 어떠한 값을 지니는지 모른다.
- 동전을 두번 던졌는데, 두번 다 앞면이 나왔다. 그렇다면 $\theta$ 값에 대한 우리의 믿음은 어떻게 변하게 되는가?
  - $\pi(\theta) = 1 (0 \leq \theta \leq 1)$ : Uniform 분포의 pdf 
  - $p(y \vert \theta) \propto \theta^{y}(1-\theta)^{n-y}$  : 관측 2번 중 2번이 앞면이 나왔기 때문에 n 과 y 는 모두 2
  - $p(\theta \vert y ) \propto  \theta^{y} (1-\theta)^{n-y} \propto Beta(y+1, n-y+1)$: 해당 분포는 이항분포 말고도 베타 분포의 표현이라 생각해도 된다. 

 $p(y \vert \theta)$ 이 식의 의미는 난해할 수 있지만 잘 생각해보면 동전의 앞면이 두 번 나온 결과를 우리는 어떤 사전 분포를 정해두고 그 분포에서 나온 결과값이라고 해석할 수 있다.

그러나 그 사전분포 $\pi(\theta)$ 는 우리가 전혀 알 수 없다고 이야기 했기 때문에 0과 1 사이의 Unifom 분포가 됐다고 생각 할 뿐이다.

파이썬 코드를 통해 해당 과정을 실습해보자.

```python
%matplotlib inline
from IPython.core.pylabtools import figsize
import numpy as np
from matplotlib import pyplot as plt
figsize(5,3)
import scipy.stats as stats

dist = stats.beta # posterior density 
n_trial = 2 # 2번 시행 
np.random.seed(seed=0) # random seed 고정
data = stats.bernoulli.rvs(0.5, size=n_trial) # Bernoulli(0.5)에서 random number 추출
print(data) # [1,1]
```

 

```python
x = np.linspace(0, 1, 100)
heads = data.sum() # 앞면의 개수, 관측값 y
y = dist.pdf(x, 1 + heads, 1 + n_trial - heads) # 데이터를 사용하여 posterior density 계산 
# theta 의 사후분포 유도
# uniform 이라는 가정에서 y 라는 데이터를 갖고 그려보니 앞면이 나올 확률이 1 일 때 확률이 가장 높을 것이라고
# 생각한다.
plt.plot(x, y, label="observe %d tosses,\n %d heads" % (n_trial, heads))
plt.fill_between(x, 0, y, color="#348ABD", alpha=0.4)
plt.vlines(0.5, 0, 4, color="k", linestyles="--", lw=1)
plt.xlabel("$p$, probability of heads") 
plt.show()
# Beta 분포일 것이라는 것이 계산을 통해 나왔고 거기에 data 인 y 값을 추가하여 그래프를 보니 1쪽으로 치우쳐져있다.
```

<p align = "center">
  <img width = "400" src = "https://github.com/skdytpq/skdytpq.github.io/blob/master/_pics/bayse/bayes1/1.png?raw=true">
  <br>
  그림1
</p>

plotting 한 그림은 다음과 같이 생기게 된다.

처음에 우리는 Uniform 한 사전분포로서 $\theta$ 를 가정하였는데, 결과값을 토대로 $\theta$ 에 대한 분포를 다시한번 확인하니 분포가 우상향 하는 것을 확인할 수 있다.

만약 동전을 던지는 시행 횟수를 늘린다면 어떻게 될 것인가?

```python
n_trial = 10
np.random.seed(seed=0) # random seed 고정
data = stats.bernoulli.rvs(0.5, size=n_trial) # Bernoulli(0.5)에서 random number 추출

x = np.linspace(0, 1, 100)
heads = data.sum() # 앞면의 개수 
y = dist.pdf(x, 1 + heads, 1 + n_trial - heads) # 데이터를 사용하여 posterior density 계산 
# 사후분포를 그려본 것. dist.pdf 를 통해 베타분포를 그림

plt.plot(x, y, label="observe %d tosses,\n %d heads" % (n_trial, heads))
plt.fill_between(x, 0, y, color="#348ABD", alpha=0.4)
plt.vlines(0.5, 0, 4, color="k", linestyles="--", lw=1)
plt.xlabel("$p$, probability of heads") 
plt.show()
```

<p align = "center">
  <img width = "400" src = "https://github.com/skdytpq/skdytpq.github.io/blob/master/_pics/bayse/bayes1/2.png?raw=true">
  <br>
  그림2
</p>

위 코드를 실행하게 된다면 위와 같은 형태의 분포를 확인하게 된다. 

앞서 우리는 극단적으로 시행횟수가 2 이고 둘 다 앞면이 나온 경우에서 사전 분포에 대한 믿음의 변화를 살펴보았는데, 위 경우는 10번의 시행으로 실제 데이터를 더 수집한 경우이다.

이 경우 앞면이 나올 확률이 0.5 근처에서 가장 높은 종 모양과 비슷하게 생긴 분포를 확인할 수 있다. 이 결과는 우리가 당연하게 생각하는 $p(\theta) = 0.5$ 라는 믿음과 좀 더 맞는 분포라고 할 수 있다.

다시 말해 데이터를 좀 더 모을수록 사후분포는 좀 더 Robust 하고 정합적인 분포를 띄는 것을 확인할 수 있다. 

n_trial 을 10000으로 늘린다면 더더욱 0.5의 값이 가장 높은 종 모형의 분포가 만들어 지는 것을 확인할 수 있다.

실제 베이지안 모델링을 할 때 PYMC3 라는 모듈을 주로 사용하여 분석을 진행하게 된다. 그러나 지금과 같이 이항 분포를 우도 함수로 가정하고 사전 분포를 Uniform 혹은 Beta 분포라고 가정을 하게 되면 별도의 사후 분포 모델링 없이 계산 만으로 Beta 분포 형태의 사후 분포를 뽑아낼 수 있다.

그렇기 때문에 해당 코드에서는 y 를 계산한 결과 분포인 Beta 분포로 바로 놓아 분포를 추정하게 되었다.

물론 사전 분포와 우도 함수만으로 어떤 식을 따르는 사후 분포를 정확하게 추정하기가 거의 불가능한 경우도 있는데, 이러한 경우의 사후 분포 추정은 샘플링 파트에서 이야기 하도록 하겠다.

## 베이지안 아이디어

다시한번 베이지안 정리에 대한 아이디어를 이야기해보자.

기존 조건부 확률의 관점을 뒤집어서 생각하는 것이 베이지안 정리의 아이디어이다.

우리는 실제 세계에서 일어나는 일들에 대해 정확하게 어떠한 분포를 따르는지 말할 수 없는 경우가 대부분이다.

하지만 그러한 일들의 결과에 대한 데이터는 충분히 모을 수 있다.

만약 우리가 정말로 알고싶은 어떤 사건의 분포를 $\pi(\theta)$ 라고 했을때, 우리는 우리가 수집한 결과 데이터 $P(X \vert \theta)$ 를 수집하고 모델링 함으로써 해당 데이터를 관찰한 후에 $\theta$ 의 변화 다시말해 $P(\theta \vert X)$ 를 알 수 있게 되는 것이다.

이러한 추정은 매우 많은 부분에서 쓰이며 딥러닝, 데이터 분석과 같은 분야에 매우 범용적으로 사용되게 되는 아이디어 이다.

이후 게시글들은 이러한 베이지안의 아이디어를 좀 더 정교화 시키고 다른 방식으로 이해해보는 파트이다.

베이지안의 아이디어를 확실하게 이해하는 것은 매우 중요한 강점이 될 수 있을 것 같다.
